---
title: "Integration of EM Algorithm in Mode Detection for Gaussian Mixture Models"
subtitle: "On The Theory and Application of Gaussian Mixture Model"
author: "Pearllyn Clavecillas"
date: "2024-09-20"
---
***CHAPTER 1: INTRODUCTION***

This chapter provides an overview of Expectation-Maximization (EM) algorithm, focusing on their theoretical framework and it's integration to Gaussian Mixture Models. It emphasizes the importance of EM in mode detection and Gaussian Mixture Model. This introduction offers
valuable insights into how EM can be effectively used in GMMs.

**1.1 Background of the Study**

  The Expectation-Maximization (EM) algorithm is a powerful iterative method developed for parameter estimation in models involving latent (unobserved) variables. It is introduced by Dempster, Laird, and Rubin in 1977, the EM algorithm was designed to handle incomplete data, which makes it widely applicable in fields ranging from statistics to machine learning. It works by finding maximum likelihood estimates of parameters in probabilistic models, where the data depends on unobserved latent variables (Dempster et.al 1977).

  The key components of the EM Algorithm are the Expectation Step known as E-Step and Maximization Step also known as M-Step. In Expectation Step, the algorithm calculates the expected value of the log-likelihood function assuming current estimates of the parameters. Since some data is missing or latent, the algorithm fills in this missing information by computing expected values based on the current parameter estimates. While in Maximization step, the parameters are updated to maximize the expected log-likelihood computed in E-Step. This step refines the parameters to increase the likelihood of the observed data given the current estimates of the latent variable (Dempster et. al 1977).
  
  The EM algorithm repeats the E and M steps iteratively. Each iteration of these steps is guaranteed to improve or maintain the likelihood of the observed data. Convergence is achieved when the change in parameter estimates between iterations is sufficiently small. While the algorithm doesn't always converge to the global maximum, it reliably finds a local maximum of the likelihood function, which is often sufficient for practical applications (Bishop 2006).
  
  The EM algorithm is particularly valuable for mixture models such as Gaussian Mixture Models. The EM algorithm provides a framework for GMMs to iteratively adjust parameters, allowing for more effective clustering, density estimation, and probabilistic modeling by optimizing the fit of each Gaussian component to the data (Carreira-Perpinan 2000). 
  
  In many clustering applications, like customer segmentation in marketing or cell-type identification in genomics, GMMs are used because they provide probabilistic assignments. The EM algorithm calculates responsibilities in each iteration, which express the probability that a data point belongs to each cluster (or Gaussian component). This soft assignment, achieved through the E-Step, allows more realistic clustering where each point can belong to multiple clusters to varying degrees.While in the M-Step, the EM algorithm adjusts each cluster's parameters (mean, variance, and weight) based on the computed responsibilities, allowing each Gaussian component to fit the data distribution more accurately with each iteration. This iterative refinement improves the clarity of clusters, particularly in complex data with overlapping clusters (McLachlan & Krishnan 2008).


**1.2 Statement of the Problem**

**1.3 Objectives of the Study**
1. Explore the theoritical aaspect of EM Algorithm.
2. Investigate its integration to mode detection in Gaussian Mixture Models.

**1.4 Significance of the Study**

**1.5 Scope and Limitation**


**References**

Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1), 1-38. DOI: 10.1111/j.2517-6161.1977.tb01600.x.

Source: "A Unified View of the EM Algorithm and Its Role in Clustering with Gaussian Mixtures," in Journal of Machine Learning Research, by Geoffrey J. McLachlan and Thriyambakam Krishnan, 2008.
