---
title: "Thesis2k24"
listing:
  contents: posts
  sort: "date desc"
  type: default
  categories: true
  sort-ui: false
  filter-ui: false
page-layout: full
title-block-banner: true
---
**MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION**

**Gaussian Mixture Model (GMM)**

MLE of Gaussian Mixture Model (GMM)

Gaussian Mixture Model is based on the assumption that the data is generated from a mixture of several Gaussian (normal) distributions. Each Gaussian in the mixture represents a cluster or subpopulation in the data.

For a Gaussian Mixture Model (GMM) with $K$ components, the probability density function (pdf) of a data point $x$ is:

$$
p(x|\Theta) = \sum_{k=1}^K \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)
$$

where: 

$\pi_k$ - is the mixing weight of the $k$-th component, with $\pi_k\geq0$ and $(\sum_{k=1}^K \pi_k=1)$ .

$\mathcal{N}(x|\mu_k, \Sigma_k))$ - is the Gaussian pdf for the $k$-th component:

$$
\mathcal{N}(x \mid \mu_k, \Sigma_k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right)
$$

$K$ is the total number of Gaussian Components.

Note:

$\mu_k$ - is the mean vector of the $k$ - th Gaussian.

$\sum_k$ - is the covariance matrix of the $k$ - th Gaussian

Likelihood Function

Given a dataset ${x_1,x_2,\ldots,x_n}$, the likelihood function $\mathcal{L}(\theta)$ is the joint probability of observing the entire dataset:

$$
\mathcal{L}(\Theta) = \prod_{i=1}^N p(x_i|\Theta)
$$

Substituting the pdf of the GMM we have:

$$
\mathcal{L}(\Theta) = \prod_{i=1}^N \left( \sum_{k=1}^K \pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k) \right)
$$

Log-Likelihood Function

To simplify the optimization process, we use the log-likelihood function, which is the logarithm of the likelihood function. Taking the logarithm we have:

$$
\mathcal{L}(\Theta) = \log \left( \prod_{i=1}^N \left( \sum_{k=1}^K \pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k) \right) \right)
$$

Using the property of logarithms $(\log(ab)=\log(a)+\log(b))$:

$$
\mathcal{L}(\Theta) = \sum_{i=1}^N \log \left( \sum_{k=1}^K \pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k) \right)
$$

Expectation-Maximization (EM) Algorithm

To find the MLE of the parameters (means, covariances, and mixing weights) that maximize the likelihood of the observed data we use the Expectation-Maximization (EM) algorithm. The EM algorithm iterates between two steps:

E-Step (Expectation Step)

The E-step computes the responsibility $\gamma_{ik}$​, which is the probability that data point $\mathbf{x}_k$​ was generated by the $k$-th Gaussian component. Mathematically, the responsibility is given by the conditional probability:

$$
\gamma_{ik} = P(z_i = k|\mathbf{x}_i)
$$

where:

$z_i$ is the latent variable indicating the component responsible for generating $\mathbf{x}_i$,

$P(z_i=k|\mathbf{x}_i)$ is the posterior probability that component $k$ is responsible for $\mathbf{x}_i$​.

By Bayes' theorem, the responsibility $\gamma_{ik}$​ can be written as:

$$
\gamma_{ik} =\frac{P(z_i=k)P(\mathbf{x}_i|z_i=k)}{P(\mathbf{x}_i)}
$$​

Where:

$P(z_i=k)=\pi_k$ is the prior probability of selecting the kkk-th component (the mixing weight),

$P(\mathbf{x}i|z_i=k)=\mathcal{N}(\mathbf{x}i|\mu_k, \Sigma_k)$ is the likelihood of the data point $\mathbf{x}_i$ under component $k$,

$P(\mathbf{x}_i)$  is the marginal likelihood, or the total probability of observing $\mathbf{x}_i$​, which is a weighted sum of the likelihoods over all components.

Then expressing the marginal likelihood $P(\mathbf{x}_i)$ as:

$$
P(\mathbf{x}_i) = \Sigma_{j=1}^K P(z_i=j)P(\mathbf{x}_i|z_i=j)
$$

 and the Gaussian likelihoods, we get:

$$
P(\mathbf{x}_i)=\Sigma_{j=1}^K \pi_j\mathcal{N}(\mathbf{x}_i|\mu_j, \Sigma_j)
$$

Now, we substitute the expressions for $P(z_i=k), P(\mathbf{x}i|zi=k)$ , and $P(\mathbf{x}_i)$ into the equation for $\gamma_{ik}$​:

$$
\gamma_{ik} =\frac{\pi_k\mathcal{N}(\mathbf{x}_i|\mu_k, \Sigma_k)}{\Sigma_{j=1}^K\mathcal{N}(\mathbf{x}_i|\mu_i,\Sigma_j)}
$$

Hence, this is the responsibility $\gamma_{ik}$​, which represents the posterior probability that data point $\mathbf{x}_i$​ was generated by component $k$.

M-Step (Maximization Step)

Updating the parameters $(\pi_k)$, $(\mu_k)$, and $(\Sigma_k)$ using the responsibilities $(\gamma_{ik})$ :

Update Mixing Weights

The mixing weight $(\pi_k)$ is updated as:

$$
\pi_k = \frac{1}{N} \sum_{i=1}^N \gamma_{ik}
$$

In a GMM, the data is assumed to be generated from a mixture of $K$ Gaussian distributions. Each Gaussian component $k$ has an associated mixing weight $\pi_k$​. These weights determine how much each Gaussian component contributes to the overall mixture model.

Mathematical Representation: The mixing weight $\pi_k$ is a non-negative value that satisfies: 

$$
\pi_k\geq0
$$

and

$$
\sum_{k-1}^K \pi_k=1
$$

This ensures that the weights are properly normalized to sum to 1, representing a probability distribution over the components.

Update Means

The mean $(\mu_k)$ is updated as:

$$
\mu_k = \frac{\sum_{i=1}^N \gamma_{ik} x_i}{\sum_{i=1}^N \gamma_{ik}}
$$

where:

$\gamma_{ik}$ is the responsibility of component $k$ for data point $x_i$​, calculated in the E-step.

The numerator is the weighted sum of the data points, and the denominator is the sum of the responsibilities for component $k$.

Update Covariances

The covariance $(\Sigma_k)$ is updated as:

$$
\Sigma_k = \frac{\sum_{i=1}^N \gamma_{ik} (x_i - \mu_k) (x_i - \mu_k)^T}{\sum_{i=1}^N \gamma_{ik}}
$$

Where:

$\gamma_{ik}$ is the responsibility that component $k$ has for data point $x_i$​, calculated in the E-step.

$(x_i - \mu_k)$ is the difference between the data point and the mean of the kkk-th component.

The numerator represents the weighted sum of the outer products of the difference vectors $(x_i - \mu_k)$, which captures both the variance and covariance of the data assigned to the $k$-th component.

The denominator is the sum of the responsibilities for component $k$, which normalizes the weighted sum.

In conclusion, to get the MLE of the Gaussian Mixture Model we use the Expectation-Maximization Algorithm. This algorithm is particularly useful for GMMs because it can handle the uncertainty about which Gaussian Component generated each data point, leading to soft clusteringg and more flexible modeling of complex data distributions.





