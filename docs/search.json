[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thesis2k24",
    "section": "",
    "text": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION\nGaussian Mixture Model (GMM)\nMLE of Gaussian Mixture Model (GMM)\nGaussian Mixture Model is based on the assumption that the data is generated from a mixture of several Gaussian (normal) distributions. Each Gaussian in the mixture represents a cluster or subpopulation in the data.\nFor a Gaussian Mixture Model (GMM) with \\(K\\) components, the probability density function (pdf) of a data point \\(x\\) is:\n\\[\np(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n\\]\nwhere:\n\\(\\pi_k\\) - is the mixing weight of the \\(k\\)-th component, with \\(\\pi_k\\geq0\\) and \\((\\sum_{k=1}^K \\pi_k=1)\\) .\n\\(\\mathcal{N}(x|\\mu_k, \\Sigma_k))\\) - is the Gaussian pdf for the \\(k\\)-th component:\n\\[\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right)\n\\]\n\\(K\\) is the total number of Gaussian Components.\nNote:\n\\(\\mu_k\\) - is the mean vector of the \\(k\\) - th Gaussian.\n\\(\\sum_k\\) - is the covariance matrix of the \\(k\\) - th Gaussian\nLikelihood Function\nGiven a dataset \\({x_1,x_2,\\ldots,x_n}\\), the likelihood function \\(\\mathcal{L}(\\theta)\\) is the joint probability of observing the entire dataset:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N p(x_i|\\Theta)\n\\]\nSubstituting the pdf of the GMM we have:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nLog-Likelihood Function\nTo simplify the optimization process, we use the log-likelihood function, which is the logarithm of the likelihood function. Taking the logarithm we have:\n\\[\n\\mathcal{L}(\\Theta) = \\log \\left( \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right) \\right)\n\\]\nUsing the property of logarithms \\((\\log(ab)=\\log(a)+\\log(b))\\):\n\\[\n\\mathcal{L}(\\Theta) = \\sum_{i=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nExpectation-Maximization (EM) Algorithm\nTo find the MLE of the parameters (means, covariances, and mixing weights) that maximize the likelihood of the observed data we use the Expectation-Maximization (EM) algorithm. The EM algorithm iterates between two steps:\nE-Step (Expectation Step)\nThe E-step computes the responsibility \\(\\gamma_{ik}\\)​, which is the probability that data point \\(\\mathbf{x}_k\\)​ was generated by the \\(k\\)-th Gaussian component. Mathematically, the responsibility is given by the conditional probability:\n\\[\n\\gamma_{ik} = P(z_i = k|\\mathbf{x}_i)\n\\]\nwhere:\n\\(z_i\\) is the latent variable indicating the component responsible for generating \\(\\mathbf{x}_i\\),\n\\(P(z_i=k|\\mathbf{x}_i)\\) is the posterior probability that component \\(k\\) is responsible for \\(\\mathbf{x}_i\\)​.\nBy Bayes’ theorem, the responsibility \\(\\gamma_{ik}\\)​ can be written as:\n\\[\n\\gamma_{ik} =\\frac{P(z_i=k)P(\\mathbf{x}_i|z_i=k)}{P(\\mathbf{x}_i)}\n\\]​\nWhere:\n\\(P(z_i=k)=\\pi_k\\) is the prior probability of selecting the kkk-th component (the mixing weight),\n\\(P(\\mathbf{x}i|z_i=k)=\\mathcal{N}(\\mathbf{x}i|\\mu_k, \\Sigma_k)\\) is the likelihood of the data point \\(\\mathbf{x}_i\\) under component \\(k\\),\n\\(P(\\mathbf{x}_i)\\) is the marginal likelihood, or the total probability of observing \\(\\mathbf{x}_i\\)​, which is a weighted sum of the likelihoods over all components.\nThen expressing the marginal likelihood \\(P(\\mathbf{x}_i)\\) as:\n\\[\nP(\\mathbf{x}_i) = \\Sigma_{j=1}^K P(z_i=j)P(\\mathbf{x}_i|z_i=j)\n\\]\nand the Gaussian likelihoods, we get:\n\\[\nP(\\mathbf{x}_i)=\\Sigma_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}_i|\\mu_j, \\Sigma_j)\n\\]\nNow, we substitute the expressions for \\(P(z_i=k), P(\\mathbf{x}i|zi=k)\\) , and \\(P(\\mathbf{x}_i)\\) into the equation for \\(\\gamma_{ik}\\)​:\n\\[\n\\gamma_{ik} =\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_i|\\mu_k, \\Sigma_k)}{\\Sigma_{j=1}^K\\mathcal{N}(\\mathbf{x}_i|\\mu_i,\\Sigma_j)}\n\\]\nHence, this is the responsibility \\(\\gamma_{ik}\\)​, which represents the posterior probability that data point \\(\\mathbf{x}_i\\)​ was generated by component \\(k\\).\nM-Step (Maximization Step)\nUpdating the parameters \\((\\pi_k)\\), \\((\\mu_k)\\), and \\((\\Sigma_k)\\) using the responsibilities \\((\\gamma_{ik})\\) :\nUpdate Mixing Weights\nThe mixing weight \\((\\pi_k)\\) is updated as:\n\\[\n\\pi_k = \\frac{1}{N} \\sum_{i=1}^N \\gamma_{ik}\n\\]\nIn a GMM, the data is assumed to be generated from a mixture of \\(K\\) Gaussian distributions. Each Gaussian component \\(k\\) has an associated mixing weight \\(\\pi_k\\)​. These weights determine how much each Gaussian component contributes to the overall mixture model.\nMathematical Representation: The mixing weight \\(\\pi_k\\) is a non-negative value that satisfies:\n\\[\n\\pi_k\\geq0\n\\]\nand\n\\[\n\\sum_{k-1}^K \\pi_k=1\n\\]\nThis ensures that the weights are properly normalized to sum to 1, representing a probability distribution over the components.\nUpdate Means\nThe mean \\((\\mu_k)\\) is updated as:\n\\[\n\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nwhere:\n\\(\\gamma_{ik}\\) is the responsibility of component \\(k\\) for data point \\(x_i\\)​, calculated in the E-step.\nThe numerator is the weighted sum of the data points, and the denominator is the sum of the responsibilities for component \\(k\\).\nUpdate Covariances\nThe covariance \\((\\Sigma_k)\\) is updated as:\n\\[\n\\Sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k) (x_i - \\mu_k)^T}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nWhere:\n\\(\\gamma_{ik}\\) is the responsibility that component \\(k\\) has for data point \\(x_i\\)​, calculated in the E-step.\n\\((x_i - \\mu_k)\\) is the difference between the data point and the mean of the kkk-th component.\nThe numerator represents the weighted sum of the outer products of the difference vectors \\((x_i - \\mu_k)\\), which captures both the variance and covariance of the data assigned to the \\(k\\)-th component.\nThe denominator is the sum of the responsibilities for component \\(k\\), which normalizes the weighted sum.\nIn conclusion, to get the MLE of the Gaussian Mixture Model we use the Expectation-Maximization Algorithm. This algorithm is particularly useful for GMMs because it can handle the uncertainty about which Gaussian Component generated each data point, leading to soft clusteringg and more flexible modeling of complex data distributions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 17, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 14, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]