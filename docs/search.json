[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thesis2k24",
    "section": "",
    "text": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\n\n\n\n\n\n\nCHAPTER 1:INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "",
    "text": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION\nGaussian Mixture Model (GMM)\nMLE of Gaussian Mixture Model (GMM)\nGaussian Mixture Model is based on the assumption that the data is generated from a mixture of several Gaussian (normal) distributions. Each Gaussian in the mixture represents a cluster or subpopulation in the data.\nFor a Gaussian Mixture Model (GMM) with \\(K\\) components, the probability density function (pdf) of a data point \\(x\\) is:\n\\[\np(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n\\]\nwhere:\n\\(\\pi_k\\) - is the mixing weight of the \\(k\\)-th component, with \\(\\pi_k\\geq0\\) and \\((\\sum_{k=1}^K \\pi_k=1)\\) .\n\\(\\mathcal{N}(x|\\mu_k, \\Sigma_k))\\) - is the Gaussian pdf for the \\(k\\)-th component:\n\\[\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right)\n\\]\n\\(K\\) is the total number of Gaussian Components.\nNote:\n\\(\\mu_k\\) - is the mean vector of the \\(k\\) - th Gaussian.\n\\(\\sum_k\\) - is the covariance matrix of the \\(k\\) - th Gaussian\nLikelihood Function\nGiven a dataset \\({x_1,x_2,\\ldots,x_n}\\), the likelihood function \\(\\mathcal{L}(\\theta)\\) is the joint probability of observing the entire dataset:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N p(x_i|\\Theta)\n\\]\nSubstituting the pdf of the GMM we have:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nLog-Likelihood Function\nTo simplify the optimization process, we use the log-likelihood function, which is the logarithm of the likelihood function. Taking the logarithm we have:\n\\[\n\\mathcal{L}(\\Theta) = \\log \\left( \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right) \\right)\n\\]\nUsing the property of logarithms \\((\\log(ab)=\\log(a)+\\log(b))\\):\n\\[\n\\mathcal{L}(\\Theta) = \\sum_{i=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nExpectation-Maximization (EM) Algorithm\nTo find the MLE of the parameters (means, covariances, and mixing weights) that maximize the likelihood of the observed data we use the Expectation-Maximization (EM) algorithm. The EM algorithm iterates between two steps:\nE-Step (Expectation Step)\nThe E-step computes the responsibility \\(\\gamma_{ik}\\)​, which is the probability that data point \\(\\mathbf{x}_k\\)​ was generated by the \\(k\\)-th Gaussian component. Mathematically, the responsibility is given by the conditional probability:\n\\[\n\\gamma_{ik} = P(z_i = k|\\mathbf{x}_i)\n\\]\nwhere:\n\\(z_i\\) is the latent variable indicating the component responsible for generating \\(\\mathbf{x}_i\\),\n\\(P(z_i=k|\\mathbf{x}_i)\\) is the posterior probability that component \\(k\\) is responsible for \\(\\mathbf{x}_i\\)​.\nBy Bayes’ theorem, the responsibility \\(\\gamma_{ik}\\)​ can be written as:\n\\[\n\\gamma_{ik} =\\frac{P(z_i=k)P(\\mathbf{x}_i|z_i=k)}{P(\\mathbf{x}_i)}\n\\]​\nWhere:\n\\(P(z_i=k)=\\pi_k\\) is the prior probability of selecting the kkk-th component (the mixing weight),\n\\(P(\\mathbf{x}i|z_i=k)=\\mathcal{N}(\\mathbf{x}i|\\mu_k, \\Sigma_k)\\) is the likelihood of the data point \\(\\mathbf{x}_i\\) under component \\(k\\),\n\\(P(\\mathbf{x}_i)\\) is the marginal likelihood, or the total probability of observing \\(\\mathbf{x}_i\\)​, which is a weighted sum of the likelihoods over all components.\nThen expressing the marginal likelihood \\(P(\\mathbf{x}_i)\\) as:\n\\[\nP(\\mathbf{x}_i) = \\Sigma_{j=1}^K P(z_i=j)P(\\mathbf{x}_i|z_i=j)\n\\]\nand the Gaussian likelihoods, we get:\n\\[\nP(\\mathbf{x}_i)=\\Sigma_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}_i|\\mu_j, \\Sigma_j)\n\\]\nNow, we substitute the expressions for \\(P(z_i=k), P(\\mathbf{x}i|zi=k)\\) , and \\(P(\\mathbf{x}_i)\\) into the equation for \\(\\gamma_{ik}\\)​:\n\\[\n\\gamma_{ik} =\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_i|\\mu_k, \\Sigma_k)}{\\Sigma_{j=1}^K\\mathcal{N}(\\mathbf{x}_i|\\mu_i,\\Sigma_j)}\n\\]\nHence, this is the responsibility \\(\\gamma_{ik}\\)​, which represents the posterior probability that data point \\(\\mathbf{x}_i\\)​ was generated by component \\(k\\).\nM-Step (Maximization Step)\nUpdating the parameters \\((\\pi_k)\\), \\((\\mu_k)\\), and \\((\\Sigma_k)\\) using the responsibilities \\((\\gamma_{ik})\\) :\nUpdate Mixing Weights\nThe mixing weight \\((\\pi_k)\\) is updated as:\n\\[\n\\pi_k = \\frac{1}{N} \\sum_{i=1}^N \\gamma_{ik}\n\\]\nIn a GMM, the data is assumed to be generated from a mixture of \\(K\\) Gaussian distributions. Each Gaussian component \\(k\\) has an associated mixing weight \\(\\pi_k\\)​. These weights determine how much each Gaussian component contributes to the overall mixture model.\nMathematical Representation: The mixing weight \\(\\pi_k\\) is a non-negative value that satisfies:\n\\[\n\\pi_k\\geq0\n\\]\nand\n\\[\n\\sum_{k-1}^K \\pi_k=1\n\\]\nThis ensures that the weights are properly normalized to sum to 1, representing a probability distribution over the components.\nUpdate Means\nThe mean \\((\\mu_k)\\) is updated as:\n\\[\n\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nwhere:\n\\(\\gamma_{ik}\\) is the responsibility of component \\(k\\) for data point \\(x_i\\)​, calculated in the E-step.\nThe numerator is the weighted sum of the data points, and the denominator is the sum of the responsibilities for component \\(k\\).\nUpdate Covariances\nThe covariance \\((\\Sigma_k)\\) is updated as:\n\\[\n\\Sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k) (x_i - \\mu_k)^T}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nWhere:\n\\(\\gamma_{ik}\\) is the responsibility that component \\(k\\) has for data point \\(x_i\\)​, calculated in the E-step.\n\\((x_i - \\mu_k)\\) is the difference between the data point and the mean of the kkk-th component.\nThe numerator represents the weighted sum of the outer products of the difference vectors \\((x_i - \\mu_k)\\), which captures both the variance and covariance of the data assigned to the \\(k\\)-th component.\nThe denominator is the sum of the responsibilities for component \\(k\\), which normalizes the weighted sum.\nIn conclusion, to get the MLE of the Gaussian Mixture Model we use the Expectation-Maximization Algorithm. This algorithm is particularly useful for GMMs because it can handle the uncertainty about which Gaussian Component generated each data point, leading to soft clusteringg and more flexible modeling of complex data distributions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "CHAPTER 1:INTRODUCTION",
    "section": "",
    "text": "CHAPTER 1: INTRODUCTION\nA Mixture Model is a probabilistic model that represents the overall data distribution as a combination of several distinct sub-distributions, known as components. Each component corresponds to a distinct group or cluster within the data, and the overall distribution is a weighted sum of these components. The weights reflect the proportion of data points belonging to each component.\nMathematically, a mixture model assumes that the data is generated by multiple latent variables or processes, where each data point is drawn from one of the components. The most common types of mixture models are based on continuous probability distributions, like the Gaussian Mixture Model (GMM).\nThe Gaussian Mixture Model (GMM) is a probabilistic model that represents a distribution as a combination of multiple Gaussian distributions, or components. Each component in the mixture is defined by its mean, covariance, and mixing coefficient. GMM are extremely flexible in modeling complex, multimodal data distributions (Carreira-Perpiñan 2000), making them ideal for applications such as clustering, density estimation, and pattern recognition.\nOne of the main reasons why GMM are widely used in various fields is their capability to approximate any continuous distribution to a high degree of accuracy by using a sufficient number of components (Carreira-Perpiñan, 2000). For instance, in practical scenarios such as image processing or speech recognition, the Gaussian Mixture Model can effectively capture the underlying structure of data by representing different clusters or features through distinct Gaussian components (Carreira-Perpiñan, 2000).\nThe Gaussian Mixture Model (GMM) is based on the concept that the data is generated by multiple hidden factors. To determine the parameters of the mixture model (such as means, covariances, and mixing coefficients), the Expectation-Maximization (EM) algorithm is commonly used. This iterative optimization method involves estimating the expected values of the hidden variables (the E-step) and maximizing the likelihood of the observed data (the M-step) (Carreira-Perpiñan, 2000). The EM algorithm is essential in situations where direct maximization of the likelihood function is computationally challenging, enabling the GMM to progressively refine its parameters (Carreira-Perpiñan, 2000).\nGaussian Mixture Models (GMM) have practical applications and are also significant theoretically due to their universal approximation capabilities. They can model complex, multimodal distributions that arise in various settings, including mode-finding problems where the goal is to locate all the modes of a distribution (Carreira-Perpiñan, 2000). Also, this is particularly useful in tasks like object recognition and Bayesian analysis, where different modes of distribution correspond to different interpretations or hypotheses (Bishop, 2006).\nHowever, Gaussian Mixture Model (GMM) has some limitations. Despite the EM algorithm’s effectiveness in estimating GMM parameters, challenges remain in ensuring that the algorithm converges efficiently and accurately, particularly in high-dimensional and multimodal data. Moreover, while the theoretical foundation of GMM is well-established, the practical application of GMM in the classification of problems such as its role in Generative Topographic Mapping (GTM) is a potential area for further exploration.\nThus, the central problem addressed in this thesis is twofold:\nTheoretical: How can the EM algorithm be optimized for Maximum Likelihood Estimation in GMM to ensure better convergence and accuracy in parameter estimation, especially in challenging datasets?\nApplied: How can GMM be effectively applied to classification problems and be integrated into the GTM framework to enhance their utility in real-world data modeling?\nObjectives\nThe study aims to address both theoretical and practical aspects of GMM, focusing on the role of the EM algorithm in parameter estimation and the application of GMM in classification and GTM. Specifically, the objectives are to:\n\nExplore the theoretical properties of Gaussian Mixture Models (GMM), focusing on how they model multimodal data and the assumptions underlying their use.\nExamine the role of the Expectation-Maximization (EM) algorithm in Maximum Likelihood Estimation (MLE) for GMM, including its convergence properties, potential limitations, and ways to optimize its performance for large or high-dimensional datasets.\nInvestigate the application of GMM in classification problems, identifying the scenarios where GMM offers significant advantages in handling multimodal data and complex feature spaces.\nExplore the integration of GMM into the Generative Topographic Mapping (GTM) framework, examining how GMM can contribute to this.\nValidate the theoretical insights through practical applications, applying GMM to real-world classification tasks and GTM-based data analysis, and assessing their effectiveness in terms of accuracy, scalability, and computational efficiency.\n\nScope and Limitation\nThis thesis explores both the theoretical aspects and practical applications of Gaussian Mixture Model (GMM). It includes a detailed examination of GMM, covering their properties and estimation using the Expectation-Maximization (EM) algorithm for Maximum Likelihood Estimation. Special attention will be given to the behavior and performance of the EM algorithm across different datasets. The study will also investigate the use of GMM in classification problems and their integration into the Generative Topographic Mapping (GTM) framework."
  },
  {
    "objectID": "CHAPTER 1.html",
    "href": "CHAPTER 1.html",
    "title": "Thesis2k24",
    "section": "",
    "text": "CHAPTER 1: INTRODUCTION\nA Mixture Model is a probabilistic model that represents the overall data distribution as a combination of several distinct sub-distributions, known as components. Each component corresponds to a distinct group or cluster within the data, and the overall distribution is a weighted sum of these components. The weights reflect the proportion of data points belonging to each component.\nMathematically, a mixture model assumes that the data is generated by multiple latent variables or processes, where each data point is drawn from one of the components. The most common types of mixture models are based on continuous probability distributions, like the Gaussian Mixture Model (GMM).\nThe Gaussian Mixture Model (GMM) is a probabilistic model that represents a distribution as a combination of multiple Gaussian distributions, or components. Each component in the mixture is defined by its mean, covariance, and mixing coefficient. GMM are extremely flexible in modeling complex, multimodal data distributions (Carreira-Perpiñan 2000), making them ideal for applications such as clustering, density estimation, and pattern recognition.\nOne of the main reasons why GMM are widely used in various fields is their capability to approximate any continuous distribution to a high degree of accuracy by using a sufficient number of components (Carreira-Perpiñan, 2000). For instance, in practical scenarios such as image processing or speech recognition, the Gaussian Mixture Model can effectively capture the underlying structure of data by representing different clusters or features through distinct Gaussian components (Carreira-Perpiñan, 2000).\nThe Gaussian Mixture Model (GMM) is based on the concept that the data is generated by multiple hidden factors. To determine the parameters of the mixture model (such as means, covariances, and mixing coefficients), the Expectation-Maximization (EM) algorithm is commonly used. This iterative optimization method involves estimating the expected values of the hidden variables (the E-step) and maximizing the likelihood of the observed data (the M-step) (Carreira-Perpiñan, 2000). The EM algorithm is essential in situations where direct maximization of the likelihood function is computationally challenging, enabling the GMM to progressively refine its parameters (Carreira-Perpiñan, 2000).\nGaussian Mixture Models (GMM) have practical applications and are also significant theoretically due to their universal approximation capabilities. They can model complex, multimodal distributions that arise in various settings, including mode-finding problems where the goal is to locate all the modes of a distribution (Carreira-Perpiñan, 2000). Also, this is particularly useful in tasks like object recognition and Bayesian analysis, where different modes of distribution correspond to different interpretations or hypotheses (Bishop, 2006).\nHowever, Gaussian Mixture Model (GMM) has some limitations. Despite the EM algorithm’s effectiveness in estimating GMM parameters, challenges remain in ensuring that the algorithm converges efficiently and accurately, particularly in high-dimensional and multimodal data. Moreover, while the theoretical foundation of GMM is well-established, the practical application of GMM in the classification of problems such as its role in Generative Topographic Mapping (GTM) is a potential area for further exploration.\nThus, the central problem addressed in this thesis is twofold:\nTheoretical: How can the EM algorithm be optimized for Maximum Likelihood Estimation in GMM to ensure better convergence and accuracy in parameter estimation, especially in challenging datasets?\nApplied: How can GMM be effectively applied to classification problems and be integrated into the GTM framework to enhance their utility in real-world data modeling?\nObjectives\nThe study aims to address both theoretical and practical aspects of GMM, focusing on the role of the EM algorithm in parameter estimation and the application of GMM in classification and GTM. Specifically, the objectives are to:\n\nExplore the theoretical properties of Gaussian Mixture Models (GMM), focusing on how they model multimodal data and the assumptions underlying their use.\nExamine the role of the Expectation-Maximization (EM) algorithm in Maximum Likelihood Estimation (MLE) for GMM, including its convergence properties, potential limitations, and ways to optimize its performance for large or high-dimensional datasets.\nInvestigate the application of GMM in classification problems, identifying the scenarios where GMM offers significant advantages in handling multimodal data and complex feature spaces.\nExplore the integration of GMM into the Generative Topographic Mapping (GTM) framework, examining how GMM can contribute to this.\nValidate the theoretical insights through practical applications, applying GMM to real-world classification tasks and GTM-based data analysis, and assessing their effectiveness in terms of accuracy, scalability, and computational efficiency.\n\nScope and Limitation\nThis thesis explores both the theoretical aspects and practical applications of Gaussian Mixture Model (GMM). It includes a detailed examination of GMM, covering their properties and estimation using the Expectation-Maximization (EM) algorithm for Maximum Likelihood Estimation. Special attention will be given to the behavior and performance of the EM algorithm across different datasets. The study will also investigate the use of GMM in classification problems and their integration into the Generative Topographic Mapping (GTM) framework."
  },
  {
    "objectID": "Chapter 1-Introduction.html",
    "href": "Chapter 1-Introduction.html",
    "title": "CHAPTER 1",
    "section": "",
    "text": "CHAPTER 1: INTRODUCTION\nA Mixture Model is a probabilistic model that represents the overall data distribution as a combination of several distinct sub-distributions, known as components. Each component corresponds to a distinct group or cluster within the data, and the overall distribution is a weighted sum of these components. The weights reflect the proportion of data points belonging to each component.\nMathematically, a mixture model assumes that the data is generated by multiple latent variables or processes, where each data point is drawn from one of the components. The most common types of mixture models are based on continuous probability distributions, like the Gaussian Mixture Model (GMM).\nThe Gaussian Mixture Model (GMM) is a probabilistic model that represents a distribution as a combination of multiple Gaussian distributions, or components. Each component in the mixture is defined by its mean, covariance, and mixing coefficient. GMM are extremely flexible in modeling complex, multimodal data distributions (Carreira-Perpiñan 2000), making them ideal for applications such as clustering, density estimation, and pattern recognition.\nOne of the main reasons why GMM are widely used in various fields is their capability to approximate any continuous distribution to a high degree of accuracy by using a sufficient number of components (Carreira-Perpiñan, 2000). For instance, in practical scenarios such as image processing or speech recognition, the Gaussian Mixture Model can effectively capture the underlying structure of data by representing different clusters or features through distinct Gaussian components (Carreira-Perpiñan, 2000).\nThe Gaussian Mixture Model (GMM) is based on the concept that the data is generated by multiple hidden factors. To determine the parameters of the mixture model (such as means, covariances, and mixing coefficients), the Expectation-Maximization (EM) algorithm is commonly used. This iterative optimization method involves estimating the expected values of the hidden variables (the E-step) and maximizing the likelihood of the observed data (the M-step) (Carreira-Perpiñan, 2000). The EM algorithm is essential in situations where direct maximization of the likelihood function is computationally challenging, enabling the GMM to progressively refine its parameters (Carreira-Perpiñan, 2000).\nGaussian Mixture Models (GMM) have practical applications and are also significant theoretically due to their universal approximation capabilities. They can model complex, multimodal distributions that arise in various settings, including mode-finding problems where the goal is to locate all the modes of a distribution (Carreira-Perpiñan, 2000). Also, this is particularly useful in tasks like object recognition and Bayesian analysis, where different modes of distribution correspond to different interpretations or hypotheses (Bishop, 2006).\nHowever, Gaussian Mixture Model (GMM) has some limitations. Despite the EM algorithm’s effectiveness in estimating GMM parameters, challenges remain in ensuring that the algorithm converges efficiently and accurately, particularly in high-dimensional and multimodal data. Moreover, while the theoretical foundation of GMM is well-established, the practical application of GMM in the classification of problems such as its role in Generative Topographic Mapping (GTM) is a potential area for further exploration.\nThus, the central problem addressed in this thesis is twofold:\nTheoretical: How can the EM algorithm be optimized for Maximum Likelihood Estimation in GMM to ensure better convergence and accuracy in parameter estimation, especially in challenging datasets?\nApplied: How can GMM be effectively applied to classification problems and be integrated into the GTM framework to enhance their utility in real-world data modeling?\nObjectives\nThe study aims to address both theoretical and practical aspects of GMM, focusing on the role of the EM algorithm in parameter estimation and the application of GMM in classification and GTM. Specifically, the objectives are to:\n\nExplore the theoretical properties of Gaussian Mixture Models (GMM), focusing on how they model multimodal data and the assumptions underlying their use.\nExamine the role of the Expectation-Maximization (EM) algorithm in Maximum Likelihood Estimation (MLE) for GMM, including its convergence properties, potential limitations, and ways to optimize its performance for large or high-dimensional datasets.\nInvestigate the application of GMM in classification problems, identifying the scenarios where GMM offers significant advantages in handling multimodal data and complex feature spaces.\nExplore the integration of GMM into the Generative Topographic Mapping (GTM) framework, examining how GMM can contribute to this.\nValidate the theoretical insights through practical applications, applying GMM to real-world classification tasks and GTM-based data analysis, and assessing their effectiveness in terms of accuracy, scalability, and computational efficiency.\n\nScope and Limitation\nThis thesis explores both the theoretical aspects and practical applications of Gaussian Mixture Model (GMM). It includes a detailed examination of GMM, covering their properties and estimation using the Expectation-Maximization (EM) algorithm for Maximum Likelihood Estimation. Special attention will be given to the behavior and performance of the EM algorithm across different datasets. The study will also investigate the use of GMM in classification problems and their integration into the Generative Topographic Mapping (GTM) framework.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\n\n\n\n\n\n\nCHAPTER 1:INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\nNo matching items"
  }
]