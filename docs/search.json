[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thesis2k24",
    "section": "",
    "text": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\n\n\n\n\n\n\nIntegration of EM Algorithm in Mode Detection for Gaussian Mixture Models\n\n\nOn The Theory and Application of Gaussian Mixture Model\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "",
    "text": "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. When we say Probabilistic model it take into account the impact of random events or actions in predicting the potential occurence of future outcomes, thus it make decisions based on likelihoods and probabilities.\nGMMs being a probabilistic models, represents normally distributed subpopulations within an overall population, this gives us the idea that it may have more than two components. In fact, a Gaussian mixture model is parameterized by two types of values, the mixture component weights and the component means and variances/covariances. Estimating these said parameters is important in order to measure and diagnose the underlying true values of the population. However, estimating the parameters of the individual normal distribution components is a canonical problem in modeling data with GMMs.\nIn frequentist probability theory, models are usually learned using maximum likelihood estimation (MLE) techniques. MLE is a method that allows us to estimate the parameters of the probability distribution that generated the sample using the sample itself. It aims to maximize the probability or likelihood of the observed data given the model parameters. However, finding the maximum likelihood solution for mixture models by differentiating the log-likelihood and solving for 0 is often analytically impossible (McGonagle et. al.). This led to the development of the Expectation-Maximization (EM) algorithm, which aims to find the most likely estimates for models with latent variables, such as Gaussian mixture models.\nExpectation-Maximization algorithm is an iterative algorithm which is a numerical technique for maximum likelihood estimation and is usually used when closed form expression for updating the model parameters can be calculated. It has a convenient property that the maximum likelihood of the data strictly increases with each subsequent iteration, meaning it is guaranteed to approach a local maximum or saddle point. EM algorithm has two steps, the Expectation (E-step) and the Maximization (M-step). The E-step is consist of calculating the expectation of the component assignments for each data point given the model parameters. While the M-step consist of maximizing the expectations calculated in the E-step with respect to the model parameters. This step consist of updating the values of the parameters.\nBefore proceeding to the Maximum likelihood estimation of the Gaussian Mixture Model let’s have first a quick overview of the MLE of Normal Distribution."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Integration of EM Algorithm in Mode Detection for Gaussian Mixture Models",
    "section": "",
    "text": "CHAPTER 1: INTRODUCTION\nThis chapter offers a comprehensive overview of the Expectation-Maximization (EM) algorithm, emphasizing its methodology, derivation, and significance within Gaussian Mixture Models for mode detection applications. It underscores the importance of the EM algorithm specifically in the context of identifying modes, providing valuable insights into its effective utilization for this purpose.\n1.1 Background of the Study\nThe Expectation-Maximization (EM) algorithm is a powerful statistical method used to estimate parameters in models involving latent variables, particularly in the context of Gaussian Mixture Models (GMMs). Its primary strength lies in its ability to handle incomplete data and perform iterative refinement of model parameters, making it a valuable tool in clustering tasks, such as mode detection. Mode detection, which involves identifying the distinct peaks or high-density regions in a dataset, is crucial in various fields such as data mining, image processing, and machine learning (Dempster et al., 1977).\nThe Expectation-Maximization (EM) algorithm, first proposed by Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin (1977), offers a robust approach for uncovering modes in complex, multimodal distributions, particularly by utilizing Gaussian Mixture Models (GMMs) as a framework for clustering and parameter estimation (McLachlan & Krishnan, 2007) The algorithm iterates between two steps: the E-step (Expectation step), where the probability of each data point belonging to a particular component of the mixture is calculated, and the M-step (Maximization step), which refines the parameters of the Gaussian components to maximize the likelihood of the observed data. This iterative process ensures that the GMM converges to an optimal representation of the data, thereby uncovering the underlying modes (Dempster et al., 1977; Carreira-Perpinan 2000).\nIn the context of mode detection, the EM algorithm plays a pivotal role in clustering the data into distinct regions based on their density. Each Gaussian component in a GMM corresponds to a mode in the data, with the algorithm refining the estimates of the means, variances, and weights of the Gaussian distributions. By doing so, it effectively separates the data into clusters or modes, which are high-density regions that represent different sub-populations within the data (McLachlan & Krishnan, 2007; Banfield & Raftery, 1993).\nThe versatility of the EM algorithm is evident in its widespread application in clustering, particularly in detecting modes in complex datasets. It has been successfully applied in fields ranging from biology to machine learning, where identifying modes is crucial for understanding the underlying structure of the data (Carreira-Perpinan 2000). Stephens (2023) further emphasizes the effectiveness of EM in identifying modes in data, noting its key role in refining the estimates of GMM parameters through iterative optimization, leading to more accurate identification of the data’s underlying modes.\nGiven the growing importance of mode detection in various research areas in our program of study, understanding the application of the EM algorithm in this context is crucial. This study aims to explore the integration of the EM algorithm in mode detection, with a focus on Gaussian Mixture Models, and its potential applications in clustering and pattern recognition.\n1.2 Statement of the Problem\nThe Expectation-Maximization (EM) algorithm has proven to be a powerful tool in statistical analysis, particularly in scenarios involving latent variables, where direct observations are incomplete or hidden. Despite its widespread use, the theoretical foundations and procedural details of the EM algorithm remain insufficiently explored in the undergraduate level of our department, especially in the context of mode detection. Mode detection, which refers to identifying the peaks or clusters in a probability distribution, is a critical task in many machine learning and statistical applications (Carreira-Perpinan 2000). However, its connection to the EM algorithm, specifically within Gaussian Mixture Models (GMMs), has not been fully explained in terms of both theoretical derivation and practical application within the undergraduate level of our program of study here in our department.\nThis study seeks to address this gap by investigating the theoretical foundations of the EM algorithm, exploring its derivation and step-by-step procedure. Furthermore, it aims to examine how the EM algorithm can be effectively applied to mode detection within GMMs, with a focus on its role in identifying distinct high-density regions in multimodal data. By understanding the theoretical and practical aspects of the EM algorithm in mode detection, this study intends to provide insights into its effectiveness and offer a deeper understanding of its applications in statistical modeling and clustering.\n1.3 Objectives of the Study\n\n. Explore the theoretical aspects such as the derivation and procedure of the EM Algorithm.\nInvestigate its integration to mode detection in Gaussian Mixture Models.\nReveal in our department the advantages and limitations of the EM algorithm in order to guide future researchers in utilizing the algorithm for mode detection.\n\n1.4 Significance of the Study\nThe application of the Expectation-Maximization (EM) algorithm in mode detection, particularly in the context of Gaussian Mixture Models (GMMs), is highly relevant to the Department of Mathematics and Statistics, where advanced statistical methods are essential for addressing complex data analysis challenges. Mode detection, which focuses on identifying clusters or high-density regions within a dataset, is vital for improving the accuracy of clustering and classification tasks. The EM algorithm, as a tool for iterative estimation of parameters in models like GMMs, plays a crucial role in identifying and refining these modes, leading to more effective clustering solutions in both theoretical and applied research (McLachlan & Krishnan, 2007; Banfield & Raftery, 1993).\nThis study offers significant contributions to the Department by exploring the theoretical aspects of the EM algorithm, including its derivation and procedural steps. The study will serve as a resource for faculty and students alike, clarifying how the EM algorithm works and how it can be correctly implemented in mode detection tasks. With a deeper understanding of the theoretical foundations, students and researchers in the department will be better equipped to apply the EM algorithm effectively in various statistical modeling contexts.\nThe integration of the EM algorithm into mode detection within GMMs is especially important for the department, as GMMs are widely used for clustering and density estimation in multivariate statistical models. Investigating the EM algorithm’s role in refining the identification of modes will allow researchers in the department to apply this methodology in real-world data, such as in pattern recognition, market segmentation, and bioinformatics (Bishop, 2006; Stephens, 2023). By focusing on this integration, the study aims to provide valuable insights that will help guide the department’s research and enhance its contribution to the field of statistics.\nFurthermore, the study will reveal the advantages and limitations of the EM algorithm in the context of mode detection, offering practical guidance for future research within the Department of Mathematics and Statistics. While the EM algorithm is powerful, it can encounter issues like local maxima and initialization sensitivity, which may affect the quality of mode detection if not addressed properly (Dempster et al., 1977; McLachlan & Krishnan, 2007). By identifying these challenges, the study aims to help students and faculty navigate potential pitfalls, ensuring that the EM algorithm is used effectively in future statistical and machine learning applications within the department.\nThrough this focused investigation, the study will not only enhance the understanding of the EM algorithm’s theoretical and practical applications but also contribute to the department’s research excellence, providing clear insights for students and faculty engaged in advanced statistical modeling and data analysis.\n1.5 Scope and Limitation\nThis study focuses on exploring the theoretical framework and practical application of the Expectation-Maximization (EM) algorithm in mode detection within Gaussian Mixture Models (GMMs), specifically tailored to the Department of Mathematics and Statistics. The primary objectives include examining the EM algorithm’s derivation, procedural steps, and its role in clustering by identifying dense regions or modes within complex datasets. By investigating these aspects, the study aims to enhance the department’s understanding of this essential algorithm, serving as a resource for faculty and students in advancing statistical modeling methods.\nThe study also investigates how the EM algorithm supports mode detection in GMMs, an area relevant to many data analysis tasks within the department. Insights into this integration can provide practical guidance for department researchers interested in applying GMMs to statistical and machine learning problems. Additionally, the study assesses the EM algorithm’s strengths and potential limitations, providing essential context for students and researchers to make informed decisions about its use in future projects.\nHowever, this research is limited to theoretical exploration and small-scale simulations of real-world datasets. Its findings and recommendations are primarily intended for academic use within the Department of Mathematics and Statistics, the study narrows its investigation to understanding the EM algorithm in mode detection within Gaussian Mixture Models and how it applies to statistical research, providing specific insights to support the department’s academic and research growth.\nReferences\n\nMcLachlan, G., & Krishnan, T. (2007). The EM Algorithm and Extensions. Wiley-Interscience.\nBanfield, J. D., & Raftery, A. E. (1993). “Model-Based Gaussian and Non-Gaussian Clustering.” Biometrics, 49(3), 803-821.\nStephens, M. (2023). “Gaussian Mixture Models and the EM Algorithm.” In Handbook of Mixture Models (pp. 305-330).\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nDempster, A. P., Laird, N. M., & Rubin, D. B. (1977). “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological), 39(1), 1-38.\n\nCHAPTER 2: PRELIMINARY CONCEPTS\nTo explore the theoretical aspects of Expectation-Maximization (EM) Algorithm in Gaussian Mixture Models and its application on Mode detections, it is important to build a strong foundation in several preliminary concepts. The following are the preliminary concepts for this study;\nProbability Theory Random Variables\n\nDefinition 2.1 : Random Variables\n\nFor a given probability space (\\(\\Omega\\), \\(\\mathscr{A}\\), \\(P[\\cdot]\\)), a random variable, denoted by \\(X\\) or \\(X(\\cdot)\\), is a function with domain \\(\\Omega\\) and counterdomain the real line. The function \\(X(\\cdot)\\) must be such that the set \\(A_r\\), defined by \\(A_r = \\{ \\omega : X(\\omega) \\leq r \\}\\) , belongs to \\(\\mathscr{A}\\) for every real number \\(r\\).\n\nExample 2.1\n\nConsider the experiment of tossing a single coin. Let the random variable \\(X\\) denote the number of heads. \\(\\Omega = \\{ head, tail\\}\\), and \\(X(\\omega) = 1\\) if \\(\\omega = head\\), and \\(X(\\omega)=0\\) if \\(\\omega = tail\\); so, the randomm variable \\(X\\) associates a real number with each outcome of the experiment. We called \\(X\\) a random variable so mathematically speaking we should show that it satisfies the definition; that is, we should show that \\(\\{\\omega : X(\\omega) \\leq r \\}\\) belongs to \\(A\\) for every real number \\(r\\). \\(A\\) consists of the four subsets: \\(\\phi\\) , {head}, {tail}, and \\(\\Omega\\) . Now, if \\(r&lt; 0\\) , \\(\\{\\omega : X(\\omega) \\leq r \\} = \\phi\\) ; and if \\(0\\leq r&lt;1, \\{\\omega: X(\\omega) \\leq r\\} =\\){tail}; and if \\(r \\geq 1, \\{ \\omega: X(\\omega) \\leq r \\} = \\Omega =\\){head, tail}. Hence, for each \\(r\\) the set \\(\\{\\omega : X(\\omega) \\leq r\\}\\) belongs to \\(A\\); so \\(X(\\cdot)\\) is a random variable.\n\nDefinition 2.2: Continuous Random Variable\n\nA random variable \\(X\\) is called continuous if there exists a function \\(f_x(\\cdot)\\) such that \\(F_x (x) = \\int_{-\\infty}^{x} f_x(u)du\\) for every real number \\(x\\). The cumulative distribution \\(F_x (\\cdot)\\) of a continuous random variable \\(X\\) is called absolutely continuous.\n\nDefinition 2.3: Probability Density Function of a Continuous Random Variable\n\nIf \\(X\\) is a continouos random variable the \\(f_x (\\cdot)\\) in \\(F_x(x) = \\int_{-\\infty}^{x} f_x(u)du\\) is called the probability density function of \\(X\\).\n\nDefinition 2.4: Probability Density Function\n\nAny function \\(f(\\cdot)\\) with domain the real line and counter domain \\([0, \\infty]\\) is defined to be a probability density function if and only if\n(i) \\(f(x)\\geq0\\) for all \\(x\\).\n(ii) \\(\\int_{-\\infty}^{\\infty} f(x)dx = 1\\)\n\nDefinition 2.5: Mean\n\nLet \\(X\\) be a random variable. The mean of \\(X\\), denoted by \\(\\mu_x\\) or \\(\\mathscr{E}[X]\\) is defined by:\n(i) \\(\\mathscr{E}[X] =\\sum x_i f_x (x_i)\\)\nif \\(X\\) is discrete with mass points \\(x_1, x_2,\\dots , x_j,\\dots\\)\n(ii) \\(\\mathscr{E}[X] = \\int_{-\\infty}^{\\infty} xf_x(x)dx\\)\nif \\(X\\) is continuous with probability density function \\(f_x(x)\\).\n(iii) \\(\\mathscr{E}[X] = \\int_{0}^{\\infty} [1-F_x(x)]dx - \\int_{-\\infty}^{0} F_x(x)dx\\)\nfor an arbitrary random variable \\(X\\).\n\nExample 2.5\n\nLet \\(X\\) be a continuous random variable with probability density function \\(f_x(x)=\\lambda e ^{\\lambda x} I_{[0,\\infty)} (x)\\).\n\\[\n\\mathscr{E}[X] = \\int_{-\\infty}^{\\infty} xf_x(x)dx = \\int_{0}^{\\infty} x\\lambda e^{-\\lambda x} dx = \\frac{1}{\\lambda}\n\\]\nThe corresponding cumulative distribution function is\n\\[\nF_x(x) =(1-e^{\\lambda x})I_{[0,\\infty)}(x);so \\mathscr{E}[X]=\\int_{0}^{\\infty}[1-F_x(x)]dx -\\int_{-\\infty}^{0}F_x(x)dx=\\int_{0}^{\\infty}(1-1+e^{-\\lambda x})dx=\\frac{1}{\\lambda}\n\\]\n\nDefinition 2.6: Variance\n\nLet \\(X\\) be a random variable, and let \\(\\mu_x\\) be \\(\\mathscr{E}[X]\\). The variance of \\(X\\), denoted by \\(\\sigma_{x}^{2}\\) or \\(var[X]\\), is defined by\n(i) \\(var[X]=\\sum_j(x_j -\\mu_x)^2f_x(x_j)\\)\nif \\(X\\) is discrete with mass points \\(x_1, x_2,\\dots ,x_j,\\dots\\)\n(ii) \\(var[X] =\\int_{-\\infty}^{\\infty} (x-\\mu_x)^2f_x(x)dx\\)\nif \\(X\\) is continuous with probability density function \\(f_x(x)\\).\n(iii) \\(var[X]=\\int_{0}^{\\infty} 2x[1-F_x(x)+F_x(-x)]dx-\\mu_{x}^2\\)\nfor an arbitrary random variable \\(X\\).\n\nDefinition 2.7: Standard Deviation\n\nIf \\(X\\) is a random variable, the standard deviation of \\(X\\) , denoted by \\(\\sigma_x\\), is defined as \\(+\\sqrt{var[X]}\\).\n\nExample 2.7\n\n\n\nDefinition 2.8 Expectation\n\nLat \\(X\\) be a random variable and \\(g(\\cdot)\\) be a function with both domain and counterdomain the real line. The expectation or expected value of the function \\(g(\\cdot)\\) of the random variable \\(X\\), denoted by \\(\\mathscr{E}[g(X)]\\), is defined by:\n(i) \\(\\mathscr{E}[g(X)] - \\sum_j g(x_j)f_x(x_j)\\)\nif \\(X\\) is discrete with mass points \\(x_1, x_2, \\dots , x_j, \\dots\\)(provided this series is absolutely convergent.\n(ii) \\(\\mathscr{E}[g(X)]=\\int_{-\\infty}^{\\infty}g(x)f_x(x)dx\\)\nif \\(X\\) is continuous with probability density function \\(f_x(x)\\) (provided \\(\\int_{-\\infty}^{\\infty}|g(x)|f_x(x)dx&lt;\\infty\\)).*\n\nDefinition 2.8: Special Parametric Families of Univariate Distributions\n\n\n\nDefinition 2.9: Continuous Distributions\n\n\n\nDefinition 2.10: Normal Distribution\n\nA random variable \\(X\\) is defined to be normally distributed if its density is given by\n\\[\nf_x (x) = f_x (x; \\mu, \\sigma) =\n\\]\nwhere the parameters \\(\\mu\\) and \\(\\sigma\\) satisfy"
  },
  {
    "objectID": "CHAPTER 1.html",
    "href": "CHAPTER 1.html",
    "title": "Thesis2k24",
    "section": "",
    "text": "CHAPTER 1: INTRODUCTION\nA Mixture Model is a probabilistic model that represents the overall data distribution as a combination of several distinct sub-distributions, known as components. Each component corresponds to a distinct group or cluster within the data, and the overall distribution is a weighted sum of these components. The weights reflect the proportion of data points belonging to each component.\nMathematically, a mixture model assumes that the data is generated by multiple latent variables or processes, where each data point is drawn from one of the components. The most common types of mixture models are based on continuous probability distributions, like the Gaussian Mixture Model (GMM).\nThe Gaussian Mixture Model (GMM) is a probabilistic model that represents a distribution as a combination of multiple Gaussian distributions, or components. Each component in the mixture is defined by its mean, covariance, and mixing coefficient. GMM are extremely flexible in modeling complex, multimodal data distributions (Carreira-Perpiñan 2000), making them ideal for applications such as clustering, density estimation, and pattern recognition.\nOne of the main reasons why GMM are widely used in various fields is their capability to approximate any continuous distribution to a high degree of accuracy by using a sufficient number of components (Carreira-Perpiñan, 2000). For instance, in practical scenarios such as image processing or speech recognition, the Gaussian Mixture Model can effectively capture the underlying structure of data by representing different clusters or features through distinct Gaussian components (Carreira-Perpiñan, 2000).\nThe Gaussian Mixture Model (GMM) is based on the concept that the data is generated by multiple hidden factors. To determine the parameters of the mixture model (such as means, covariances, and mixing coefficients), the Expectation-Maximization (EM) algorithm is commonly used. This iterative optimization method involves estimating the expected values of the hidden variables (the E-step) and maximizing the likelihood of the observed data (the M-step) (Carreira-Perpiñan, 2000). The EM algorithm is essential in situations where direct maximization of the likelihood function is computationally challenging, enabling the GMM to progressively refine its parameters (Carreira-Perpiñan, 2000).\nGaussian Mixture Models (GMM) have practical applications and are also significant theoretically due to their universal approximation capabilities. They can model complex, multimodal distributions that arise in various settings, including mode-finding problems where the goal is to locate all the modes of a distribution (Carreira-Perpiñan, 2000). Also, this is particularly useful in tasks like object recognition and Bayesian analysis, where different modes of distribution correspond to different interpretations or hypotheses (Bishop, 2006).\nHowever, Gaussian Mixture Model (GMM) has some limitations. Despite the EM algorithm’s effectiveness in estimating GMM parameters, challenges remain in ensuring that the algorithm converges efficiently and accurately, particularly in high-dimensional and multimodal data. Moreover, while the theoretical foundation of GMM is well-established, the practical application of GMM in the classification of problems such as its role in Generative Topographic Mapping (GTM) is a potential area for further exploration.\nThus, the central problem addressed in this thesis is twofold:\nTheoretical: How can the EM algorithm be optimized for Maximum Likelihood Estimation in GMM to ensure better convergence and accuracy in parameter estimation, especially in challenging datasets?\nApplied: How can GMM be effectively applied to classification problems and be integrated into the GTM framework to enhance their utility in real-world data modeling?\nObjectives\nThe study aims to address both theoretical and practical aspects of GMM, focusing on the role of the EM algorithm in parameter estimation and the application of GMM in classification and GTM. Specifically, the objectives are to:\n\nExplore the theoretical properties of Gaussian Mixture Models (GMM), focusing on how they model multimodal data and the assumptions underlying their use.\nExamine the role of the Expectation-Maximization (EM) algorithm in Maximum Likelihood Estimation (MLE) for GMM, including its convergence properties, potential limitations, and ways to optimize its performance for large or high-dimensional datasets.\nInvestigate the application of GMM in classification problems, identifying the scenarios where GMM offers significant advantages in handling multimodal data and complex feature spaces.\nExplore the integration of GMM into the Generative Topographic Mapping (GTM) framework, examining how GMM can contribute to this.\nValidate the theoretical insights through practical applications, applying GMM to real-world classification tasks and GTM-based data analysis, and assessing their effectiveness in terms of accuracy, scalability, and computational efficiency.\n\nScope and Limitation\nThis thesis explores both the theoretical aspects and practical applications of Gaussian Mixture Model (GMM). It includes a detailed examination of GMM, covering their properties and estimation using the Expectation-Maximization (EM) algorithm for Maximum Likelihood Estimation. Special attention will be given to the behavior and performance of the EM algorithm across different datasets. The study will also investigate the use of GMM in classification problems and their integration into the Generative Topographic Mapping (GTM) framework."
  },
  {
    "objectID": "Chapter 1-Introduction.html",
    "href": "Chapter 1-Introduction.html",
    "title": "CHAPTER 1",
    "section": "",
    "text": "CHAPTER 1: INTRODUCTION\nA Mixture Model is a probabilistic model that represents the overall data distribution as a combination of several distinct sub-distributions, known as components. Each component corresponds to a distinct group or cluster within the data, and the overall distribution is a weighted sum of these components. The weights reflect the proportion of data points belonging to each component.\nMathematically, a mixture model assumes that the data is generated by multiple latent variables or processes, where each data point is drawn from one of the components. The most common types of mixture models are based on continuous probability distributions, like the Gaussian Mixture Model (GMM).\nThe Gaussian Mixture Model (GMM) is a probabilistic model that represents a distribution as a combination of multiple Gaussian distributions, or components. Each component in the mixture is defined by its mean, covariance, and mixing coefficient. GMM are extremely flexible in modeling complex, multimodal data distributions (Carreira-Perpiñan 2000), making them ideal for applications such as clustering, density estimation, and pattern recognition.\nOne of the main reasons why GMM are widely used in various fields is their capability to approximate any continuous distribution to a high degree of accuracy by using a sufficient number of components (Carreira-Perpiñan, 2000). For instance, in practical scenarios such as image processing or speech recognition, the Gaussian Mixture Model can effectively capture the underlying structure of data by representing different clusters or features through distinct Gaussian components (Carreira-Perpiñan, 2000).\nThe Gaussian Mixture Model (GMM) is based on the concept that the data is generated by multiple hidden factors. To determine the parameters of the mixture model (such as means, covariances, and mixing coefficients), the Expectation-Maximization (EM) algorithm is commonly used. This iterative optimization method involves estimating the expected values of the hidden variables (the E-step) and maximizing the likelihood of the observed data (the M-step) (Carreira-Perpiñan, 2000). The EM algorithm is essential in situations where direct maximization of the likelihood function is computationally challenging, enabling the GMM to progressively refine its parameters (Carreira-Perpiñan, 2000).\nGaussian Mixture Models (GMM) have practical applications and are also significant theoretically due to their universal approximation capabilities. They can model complex, multimodal distributions that arise in various settings, including mode-finding problems where the goal is to locate all the modes of a distribution (Carreira-Perpiñan, 2000). Also, this is particularly useful in tasks like object recognition and Bayesian analysis, where different modes of distribution correspond to different interpretations or hypotheses (Bishop, 2006).\nHowever, Gaussian Mixture Model (GMM) has some limitations. Despite the EM algorithm’s effectiveness in estimating GMM parameters, challenges remain in ensuring that the algorithm converges efficiently and accurately, particularly in high-dimensional and multimodal data. Moreover, while the theoretical foundation of GMM is well-established, the practical application of GMM in the classification of problems such as its role in Generative Topographic Mapping (GTM) is a potential area for further exploration.\nThus, the central problem addressed in this thesis is twofold:\nTheoretical: How can the EM algorithm be optimized for Maximum Likelihood Estimation in GMM to ensure better convergence and accuracy in parameter estimation, especially in challenging datasets?\nApplied: How can GMM be effectively applied to classification problems and be integrated into the GTM framework to enhance their utility in real-world data modeling?\nObjectives\nThe study aims to address both theoretical and practical aspects of GMM, focusing on the role of the EM algorithm in parameter estimation and the application of GMM in classification and GTM. Specifically, the objectives are to:\n\nExplore the theoretical properties of Gaussian Mixture Models (GMM), focusing on how they model multimodal data and the assumptions underlying their use.\nExamine the role of the Expectation-Maximization (EM) algorithm in Maximum Likelihood Estimation (MLE) for GMM, including its convergence properties, potential limitations, and ways to optimize its performance for large or high-dimensional datasets.\nInvestigate the application of GMM in classification problems, identifying the scenarios where GMM offers significant advantages in handling multimodal data and complex feature spaces.\nExplore the integration of GMM into the Generative Topographic Mapping (GTM) framework, examining how GMM can contribute to this.\nValidate the theoretical insights through practical applications, applying GMM to real-world classification tasks and GTM-based data analysis, and assessing their effectiveness in terms of accuracy, scalability, and computational efficiency.\n\nScope and Limitation\nThis thesis explores both the theoretical aspects and practical applications of Gaussian Mixture Model (GMM). It includes a detailed examination of GMM, covering their properties and estimation using the Expectation-Maximization (EM) algorithm for Maximum Likelihood Estimation. Special attention will be given to the behavior and performance of the EM algorithm across different datasets. The study will also investigate the use of GMM in classification problems and their integration into the Generative Topographic Mapping (GTM) framework.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\n\n\n\n\n\n\nIntegration of EM Algorithm in Mode Detection for Gaussian Mixture Models\n\n\nOn The Theory and Application of Gaussian Mixture Model\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPearllyn Clavecillas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#mle-of-gaussian-mixture-model",
    "href": "posts/post-with-code/index.html#mle-of-gaussian-mixture-model",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "MLE of Gaussian Mixture Model",
    "text": "MLE of Gaussian Mixture Model\nNow we attempt the same strategy for deriving the Maximum Likelihood Estimation (MLE) of the Gaussian Mixture Model (GMM). Our unknown parameters are:\n\\[\n\\theta = \\{ \\mu_1, \\dots, \\mu_K, \\sigma_1, \\dots, \\sigma_K, \\pi_1, \\dots, \\pi_K \\}\n\\]\nFrom the first section of this note, our likelihood function is:\n\\[\nL(\\theta | X_1, \\dots, X_n) = \\prod_{i=1}^n \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i; \\mu_k, \\sigma_k^2)\n\\]\nSo our log-likelihood is:\n\\[\n\\ell(\\theta) = \\sum_{i=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i; \\mu_k, \\sigma_k^2) \\right)\n\\]\nLooking at the expression above, we already notice a difference between this scenario and the simple setup in the previous section. The summation over the \\(K\\) components “blocks” our log function from being applied directly to the normal densities.\nIf we follow the same steps as before and differentiate with respect to \\(\\mu_k\\) and set the expression equal to zero, we get:\n\\[\n\\sum_{i=1}^n \\frac{1}{\\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i; \\mu_k, \\sigma_k)} \\pi_k \\mathcal{N}(x_i; \\mu_k, \\sigma_k) \\frac{(x_i - \\mu_k)}{\\sigma_k^2} = 0 \\tag{1}\n\\]\nNow, we’re stuck because we can’t analytically solve for \\(\\mu_k\\). However, we can make one important observation which provides intuition for what’s to come: if we knew the latent variables \\(Z_i\\), we could simply gather all our samples \\(X_i\\) such that \\(Z_i = k\\) and use the estimates from the previous section to estimate \\(\\mu_k\\).\n\nEM, Informally\nIntuitively, the latent variables \\(Z_i\\) should help us find the MLEs. We first attempt to compute the posterior distribution of \\(Z_i\\) given the observations:\n\\[\nP(Z_i = k | X_i) = \\frac{P(X_i | Z_i = k) P(Z_i = k)}{P(X_i)} = \\frac{\\pi_k N(\\mu_k, \\sigma^2_k)}{\\sum_{k=1}^{K} \\pi_k N(\\mu_k, \\sigma_k)} = \\gamma_{Z_i}(k) \\tag{2}\n\\]\nNow we can rewrite equation (1), the derivative of the log-likelihood with respect to \\(\\mu_k\\), as follows:\n\\[\n\\sum_{i=1}^{n} \\gamma_{Z_i}(k) (X_i - \\mu_k) \\sigma^2_k = 0\n\\]\nEven though \\(\\gamma_{Z_i}(k)\\) depends on \\(\\mu_k\\), we can cheat a bit and pretend that it doesn’t. Now we can solve for \\(\\mu_k\\) in this equation to get:\n\\[\n\\hat{\\mu}_k = \\frac{\\sum_{i=1}^{n} \\gamma_{Z_i}(k) X_i}{\\sum_{i=1}^{n} \\gamma_{Z_i}(k)} = \\frac{1}{N_k} \\sum_{i=1}^{n} \\gamma_{Z_i}(k) X_i \\tag{3}\n\\]\nWhere we set \\(N_k =\\sum_{i=1}^{n}\\gamma_{Z_i}(k)\\). We can think of \\(N_k\\) as the effective number of points assigned to component \\(k\\). We see that \\(\\hat{\\mu}_k\\) is therefore a weighted average of the data with weights \\(\\gamma_{Z_i}(k)\\).\nSimilarly, if we apply a similar method to finding \\(\\hat{\\sigma}^2_k\\) and \\(\\hat{\\pi}_k\\), we find that:\n\\[\n\\hat{\\sigma}^2_k = \\frac{1}{N_k} \\sum_{i=1}^{n} \\gamma_{Z_i}(k) (X_i - \\hat{\\mu}_k)^2 \\tag{4}\n\\]\n\\[\n\\hat{\\pi}_k = \\frac{N_k}{n} \\tag{5}\n\\]\nAgain, remember that \\(\\gamma_{Z_i}(k)\\) depends on the unknown parameters, so these equations are not closed-form expressions. This looks like a vicious circle. But, as Cosma Shalizi says, “one man’s vicious circle is another man’s successive approximation procedure.”\nWe are now in the following situation:\n\nIf we knew the parameters, we could compute the posterior probabilities \\(\\gamma_{Z_i}(k)\\).\nIf we knew the posteriors \\(\\gamma_{Z_i}(k)\\), we could easily compute the parameters.\n\nThe EM algorithm, motivated by the two observations above, proceeds as follows:\n\nInitialize the \\(\\mu_k\\), \\(\\sigma_k\\), and \\(\\pi_k\\) and evaluate the log-likelihood with these parameters.\nE-step: Evaluate the posterior probabilities \\(\\gamma_{Z_i}(k)\\) using the current values of the \\(\\mu_k\\) and \\(\\sigma_k\\) with equation (2).\nM-step: Estimate new parameters $_k $, \\(\\hat{\\sigma}^2_k\\), and \\(\\hat{\\pi}_k\\) with the current values of \\(\\gamma_{Z_i}(k)\\) using equations (3), (4), and (5).\nEvaluate the log-likelihood with the new parameter estimates. If the log-likelihood has changed by less than some small \\(\\epsilon\\), stop. Otherwise, go back to step 2.\n\nThe EM algorithm is sensitive to the initial values of the parameters, so care must be taken in the first step. However, assuming the initial values are “valid,” one property of the EM algorithm is that the log-likelihood increases at every step. This invariant proves to be useful when debugging the algorithm in practice.\n\n\nThe EM Algorithm\nThe EM algorithm attempts to find maximum likelihood estimates for models with latent variables. In this section, we describe a more abstract view of EM, which can be extended to other latent variable models.\nLet \\(X\\) be the entire set of observed variables and \\(Z\\) the entire set of latent variables. The log-likelihood is therefore:\n\\[\n\\log(P(X | \\Theta)) = \\log\\left(\\sum_Z P(X, Z | \\Theta)\\right)\n\\]\nwhere we’ve simply marginalized \\(Z\\) out of the joint distribution.\nAs we noted above, the existence of the sum inside the logarithm prevents us from applying the log to the densities, which results in a complicated expression for the MLE. Now suppose that we observed both \\(X\\) and \\(Z\\). We call \\(\\{X, Z\\}\\) the complete data set, and we say \\(X\\) is incomplete. As we noted previously, if we knew \\(Z\\), the maximization would be easy.\nWe typically don’t know \\(Z\\), but the information we do have about $Z $ is contained in the posterior \\(P(Z | X, \\Theta)\\). Since we don’t know the complete log-likelihood, we consider its expectation under the posterior distribution of the latent variables. This corresponds to the E-step above. In the M-step, we maximize this expectation to find a new estimate for the parameters.\nIn the E-step, we use the current value of the parameters \\(\\theta_0\\) to find the posterior distribution of the latent variables given by \\(P(Z | X, \\theta_0)\\). This corresponds to the \\(\\gamma_{Z_i}(k)\\) in the previous section. We then use this to find the expectation of the complete data log-likelihood, with respect to this posterior, evaluated at an arbitrary \\(\\theta\\). This expectation is denoted \\(Q(\\theta, \\theta_0)\\) and it equals:\n\\[\nQ(\\theta, \\theta_0) = \\mathbb{E}_{Z|X, \\theta_0} \\left[\\log(P(X, Z | \\theta))\\right] = \\sum_Z P(Z | X, \\theta_0) \\log(P(X, Z | \\theta))\n\\]\nIn the M-step, we determine the new parameter \\(\\hat{\\theta}\\) by maximizing \\(Q\\):\n\\[\n\\hat{\\theta} = \\arg\\max_{\\theta} Q(\\theta, \\theta_0)\n\\]\n\n\nGaussian Mixture Models\nNow we derive the relevant quantities for Gaussian mixture models and compare it to our “informal” derivation above. The complete likelihood takes the form:\n\\[\nP(X, Z | \\mu, \\sigma, \\pi) = \\prod_{i=1}^{n} \\prod_{k=1}^{K} \\pi_k^{I(Z_i = k)} N(x_i | \\mu_k, \\sigma_k)^{I(Z_i = k)}\n\\]\nso the complete log-likelihood takes the form:\n\\[\n\\log(P(X, Z | \\mu, \\sigma, \\pi)) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} I(Z_i = k) \\left( \\log(\\pi_k) + \\log(N(x_i | \\mu_k, \\sigma_k)) \\right)\n\\]\nNote that for the complete log-likelihood, the logarithm acts directly on the normal density, which leads to a simpler solution for the MLE. As we said, in practice, we do not observe the latent variables, so we consider the expectation of the complete log-likelihood with respect to the posterior of the latent variables.\nThe expected value of the complete log-likelihood is therefore:\n\\[\n\\mathbb{E}_{Z|X}\\left[\\log(P(X, Z | \\mu, \\sigma, \\pi))\\right] = \\mathbb{E}_{Z|X}\\left[\\sum_{i=1}^{n} \\sum_{k=1}^{K} I(Z_i = k) \\left( \\log(\\pi_k) + \\log(N(x_i | \\mu_k, \\sigma_k)) \\right)\\right]\n\\]\n\\[\n= \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{E}_{Z|X}[I(Z_i = k)] \\left( \\log(\\pi_k) + \\log(N(x_i | \\mu_k, \\sigma_k)) \\right)\n\\]\nSince \\(\\mathbb{E}_{Z|X}[I(Z_i = k)] = P(Z_i = k | X)\\), we see that this is simply \\(\\gamma_{Z_i}(k)\\) which we computed in the previous section. Hence, we have:\n\\[\n\\mathbb{E}_{Z|X}\\left[\\log(P(X, Z | \\mu, \\sigma, \\pi))\\right] = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\gamma_{Z_i}(k) \\left( \\log(\\pi_k) + \\log(N(x_i | \\mu_k, \\sigma_k)) \\right)\n\\]\nEM proceeds as follows: first choose initial values for \\(\\mu, \\sigma, \\pi\\) and use these in the E-step to evaluate the \\(\\gamma_{Z_i}(k)\\). Then, with \\(\\gamma_{Z_i}(k)\\) fixed, maximize the expected complete log-likelihood above with respect to \\(\\mu_k, \\sigma_k\\) and \\(\\pi_k\\). This leads to the closed-form solutions we derived in the previous section.\n\n\nExample\nIn this example, we will assume our mixture components are fully specified Gaussian distributions (i.e., the means and variances are known), and we are interested in finding the maximum likelihood estimates of the $_k $.\nAssume we have \\(K = 2\\) components, so that:\n\n\\(X_i | Z_i = 0 \\sim N(5, 1.5)\\)\n\\(X_i | Z_i = 1 \\sim N(10, 2)\\)\n\nThe true mixture proportions will be \\(P(Z_i = 0) = 0.25\\) and \\(P(Z_i = 1) = 0.75\\).\nFirst, we simulate data from this mixture model:\n\n# mixture components\nmu.true    = c(5, 10)\nsigma.true = c(1.5, 2)\n\n# determine Z_i\nZ = rbinom(500, 1, 0.75)\n# sample from mixture model\n\nX &lt;- rnorm(10000, mean=mu.true[Z+1], sd=sigma.true[Z+1])\nhist(X,breaks=15)\n\n\n\n\n\n\n\n\nNow we write a function to compute the log-likelihood for the incomplete data, assuming the parameters are known. This will be used to determine convergence:\n[ () = {i=1}^{n} ( {k=1}^{K} N(x_i; _k, ^2_k)) ]\n\ncompute.log.lik &lt;- function(L, w) {\n  L[,1] = L[,1]*w[1]\n  L[,2] = L[,2]*w[2]\n  return(sum(log(rowSums(L))))\n}\n\nSince the mixture components are fully specified, for each sample \\(X_i\\), we can compute the likelihood \\(P(X_i|Z_i=0)\\) and \\(P(X_i|Z_i=1)\\). We store these values in the columns of matrix \\(L\\):\n\nL = matrix(NA, nrow=length(X), ncol= 2)\nL[, 1] = dnorm(X, mean=mu.true[1], sd = sigma.true[1])\nL[, 2] = dnorm(X, mean=mu.true[2], sd = sigma.true[2])\n\nFinally, we implement the E and M step in the EM.iter function below. The mixture.EM function is the driver which checks for convergence by computing the log-likelihoods at each step.\n\nmixture.EM &lt;- function(w.init, L) {\n  \n  w.curr &lt;- w.init\n  \n  # store log-likehoods for each iteration\n  log_liks &lt;- c()\n  ll       &lt;- compute.log.lik(L, w.curr)\n  log_liks &lt;- c(log_liks, ll)\n  delta.ll &lt;- 1\n  \n  while(delta.ll &gt; 1e-5) {\n    w.curr   &lt;- EM.iter(w.curr, L)\n    ll       &lt;- compute.log.lik(L, w.curr)\n    log_liks &lt;- c(log_liks, ll)\n    delta.ll &lt;- log_liks[length(log_liks)]  - log_liks[length(log_liks)-1]\n  }\n  return(list(w.curr, log_liks))\n}\n\nEM.iter &lt;- function(w.curr, L, ...) {\n  \n  # E-step: compute E_{Z|X,w0}[I(Z_i = k)]\n  z_ik &lt;- L\n  for(i in seq_len(ncol(L))) {\n    z_ik[,i] &lt;- w.curr[i]*z_ik[,i]\n  }\n  z_ik     &lt;- z_ik / rowSums(z_ik)\n  \n  # M-step\n  w.next   &lt;- colSums(z_ik)/sum(z_ik)\n  return(w.next)\n}\n\n\n#perform EM\nee &lt;- mixture.EM(w.init=c(0.5,0.5), L)\nprint(paste(\"Estimate = (\", round(ee[[1]][1],2), \",\", round(ee[[1]][2],2), \")\", sep=\"\"))\n\n[1] \"Estimate = (0.23,0.77)\"\n\n\nFinally, we inspect the evolution of the log-likelihood and note that it is strictly increases:\n\nplot(ee[[2]], ylab='incomplete log-likelihood', xlab='iteration')"
  },
  {
    "objectID": "posts/post-with-code/index.html#em-informally",
    "href": "posts/post-with-code/index.html#em-informally",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "EM, informally",
    "text": "EM, informally\nMaximum Likelihood Estimation using Expectation-Maximization Algorithm\nMLE of Gaussian Mixture Model (GMM)\nGaussian Mixture Model is based on the assumption that the data is generated from a mixture of several Gaussian (normal) distributions. Each Gaussian in the mixture represents a cluster or subpopulation in the data.\nFor a Gaussian Mixture Model (GMM) with \\(K\\) components, the probability density function (pdf) of a data point \\(x\\) is:\n\\[\np(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n\\]\nwhere:\n\\(\\pi_k\\) - is the mixing weight of the \\(k\\)-th component, with \\(\\pi_k\\geq0\\) and \\((\\sum_{k=1}^K \\pi_k=1)\\) .\n\\(\\mathcal{N}(x|\\mu_k, \\Sigma_k))\\) - is the Gaussian pdf for the \\(k\\)-th component:\n\\[\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right)\n\\]\n\\(K\\) is the total number of Gaussian Components.\nNote:\n\\(\\mu_k\\) - is the mean vector of the \\(k\\) - th Gaussian.\n\\(\\sum_k\\) - is the covariance matrix of the \\(k\\) - th Gaussian\nLikelihood Function\nGiven a dataset \\({x_1,x_2,\\ldots,x_n}\\), the likelihood function \\(\\mathcal{L}(\\theta)\\) is the joint probability of observing the entire dataset:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N p(x_i|\\Theta)\n\\]\nSubstituting the pdf of the GMM we have:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nLog-Likelihood Function\nTo simplify the optimization process, we use the log-likelihood function, which is the logarithm of the likelihood function. Taking the logarithm we have:\n\\[\n\\mathcal{L}(\\Theta) = \\log \\left( \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right) \\right)\n\\]\nUsing the property of logarithms \\((\\log(ab)=\\log(a)+\\log(b))\\):\n\\[\n\\mathcal{L}(\\Theta) = \\sum_{i=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nExpectation-Maximization (EM) Algorithm\nTo find the MLE of the parameters (means, covariances, and mixing weights) that maximize the likelihood of the observed data we use the Expectation-Maximization (EM) algorithm. The EM algorithm iterates between two steps:\nE-Step (Expectation Step)\nThe E-step computes the responsibility \\(\\gamma_{ik}\\)​, which is the probability that data point \\(\\mathbf{x}_k\\)​ was generated by the \\(k\\)-th Gaussian component. Mathematically, the responsibility is given by the conditional probability:\n\\[\n\\gamma_{ik} = P(z_i = k|\\mathbf{x}_i)\n\\]\nwhere:\n\\(z_i\\) is the latent variable indicating the component responsible for generating \\(\\mathbf{x}_i\\),\n\\(P(z_i=k|\\mathbf{x}_i)\\) is the posterior probability that component \\(k\\) is responsible for \\(\\mathbf{x}_i\\)​.\nBy Bayes’ theorem, the responsibility \\(\\gamma_{ik}\\)​ can be written as:\n\\[\n\\gamma_{ik} =\\frac{P(z_i=k)P(\\mathbf{x}_i|z_i=k)}{P(\\mathbf{x}_i)}\n\\]​\nWhere:\n\\(P(z_i=k)=\\pi_k\\) is the prior probability of selecting the kkk-th component (the mixing weight),\n\\(P(\\mathbf{x}i|z_i=k)=\\mathcal{N}(\\mathbf{x}i|\\mu_k, \\Sigma_k)\\) is the likelihood of the data point \\(\\mathbf{x}_i\\) under component \\(k\\),\n\\(P(\\mathbf{x}_i)\\) is the marginal likelihood, or the total probability of observing \\(\\mathbf{x}_i\\)​, which is a weighted sum of the likelihoods over all components.\nThen expressing the marginal likelihood \\(P(\\mathbf{x}_i)\\) as:\n\\[\nP(\\mathbf{x}_i) = \\Sigma_{j=1}^K P(z_i=j)P(\\mathbf{x}_i|z_i=j)\n\\]\nand the Gaussian likelihoods, we get:\n\\[\nP(\\mathbf{x}_i)=\\Sigma_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}_i|\\mu_j, \\Sigma_j)\n\\]\nNow, we substitute the expressions for \\(P(z_i=k), P(\\mathbf{x}i|zi=k)\\) , and \\(P(\\mathbf{x}_i)\\) into the equation for \\(\\gamma_{ik}\\)​:\n\\[\n\\gamma_{ik} =\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_i|\\mu_k, \\Sigma_k)}{\\Sigma_{j=1}^K\\mathcal{N}(\\mathbf{x}_i|\\mu_i,\\Sigma_j)}\n\\]\nHence, this is the responsibility \\(\\gamma_{ik}\\)​, which represents the posterior probability that data point \\(\\mathbf{x}_i\\)​ was generated by component \\(k\\).\nM-Step (Maximization Step)\nUpdating the parameters \\((\\pi_k)\\), \\((\\mu_k)\\), and \\((\\Sigma_k)\\) using the responsibilities \\((\\gamma_{ik})\\) :\nUpdate Mixing Weights\nThe mixing weight \\((\\pi_k)\\) is updated as:\n\\[\n\\pi_k = \\frac{1}{N} \\sum_{i=1}^N \\gamma_{ik}\n\\]\nIn a GMM, the data is assumed to be generated from a mixture of \\(K\\) Gaussian distributions. Each Gaussian component \\(k\\) has an associated mixing weight \\(\\pi_k\\)​. These weights determine how much each Gaussian component contributes to the overall mixture model.\nMathematical Representation: The mixing weight \\(\\pi_k\\) is a non-negative value that satisfies:\n\\[\n\\pi_k\\geq0\n\\]\nand\n\\[\n\\sum_{k-1}^K \\pi_k=1\n\\]\nThis ensures that the weights are properly normalized to sum to 1, representing a probability distribution over the components.\nUpdate Means\nThe mean \\((\\mu_k)\\) is updated as:\n\\[\n\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nwhere:\n\\(\\gamma_{ik}\\) is the responsibility of component \\(k\\) for data point \\(x_i\\)​, calculated in the E-step.\nThe numerator is the weighted sum of the data points, and the denominator is the sum of the responsibilities for component \\(k\\).\nUpdate Covariances\nThe covariance \\((\\Sigma_k)\\) is updated as:\n\\[\n\\Sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k) (x_i - \\mu_k)^T}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nWhere:\n\\(\\gamma_{ik}\\) is the responsibility that component \\(k\\) has for data point \\(x_i\\)​, calculated in the E-step.\n\\((x_i - \\mu_k)\\) is the difference between the data point and the mean of the kkk-th component.\nThe numerator represents the weighted sum of the outer products of the difference vectors \\((x_i - \\mu_k)\\), which captures both the variance and covariance of the data assigned to the \\(k\\)-th component.\nThe denominator is the sum of the responsibilities for component \\(k\\), which normalizes the weighted sum.\nIn conclusion, to get the MLE of the Gaussian Mixture Model we use the Expectation-Maximization Algorithm. This algorithm is particularly useful for GMMs because it can handle the uncertainty about which Gaussian Component generated each data point, leading to soft clusteringg and more flexible modeling of complex data distributions."
  },
  {
    "objectID": "posts/post-with-code/index.html#gaussian-mixture-model-gmm",
    "href": "posts/post-with-code/index.html#gaussian-mixture-model-gmm",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "",
    "text": "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. When we say Probabilistic model it take into account the impact of random events or actions in predicting the potential occurence of future outcomes, thus it make decisions based on likelihoods and probabilities.\nGMMs being a probabilistic models, represents normally distributed subpopulations within an overall population, this gives us the idea that it may have more than two components. In fact, a Gaussian mixture model is parameterized by two types of values, the mixture component weights and the component means and variances/covariances. Estimating these said parameters is important in order to measure and diagnose the underlying true values of the population. However, estimating the parameters of the individual normal distribution components is a canonical in modeling data with GMMs.\nFurthermore, In frequentist probability theory, models are typically learnedby using the maximum likelihood estimation (MLE) techniques. MLE is an estimation method that allows us to use a sample to estimate the parameters of the probability distribution that generated the sample. It also seeks to maximize the probability or likelihood of the observed data given the model parameters. Unfortunately, finding the maximum likelihood solution for mixture models by differentiating the log-likelihood and solving for 0 is usually analytically impossible (McGonagle et. al). Thus, an algorithm that attempts to find maximum likelihood estimates for models with latent variables like Gaussian mixture models comes to light, called the Expectation-Maximization (EM) algorithm.\nExpectation-Maximization algorithm is an iterative algorithm which is a numerical technique for maximum likelihood estimation and is usually used when closed form expression for updating the model parameters can be calculated. It has a convenient property that the maximum likelihood of the data strictly increases with each subsequent iteration, meaning it is guaranteed to approach a local maximum or saddle point. EM algorithm has two steps, the Expectation (E-step) and the Maximization (M-step). The E-step is consist of calculating the expectation of the component assignments for each data point given the model parameters. While the M-step consist of maximizing the expectations calculated in the E-step with respect to the model parameters. This step consist of updating the values of the parameters.\nBefore proceeding to the Maximum likelihood estimation of the Gaussian Mixture Model let’s have first a quick overview of the MLE of Normal Distribution."
  },
  {
    "objectID": "posts/post-with-code/index.html#mle-of-normal-distribution",
    "href": "posts/post-with-code/index.html#mle-of-normal-distribution",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "MLE of Normal Distribution",
    "text": "MLE of Normal Distribution\nSuppose we have \\(n\\) observations \\(X_1, \\dots, X_n\\) from a Gaussian distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\). To find the maximum likelihood estimate (MLE) for \\(\\mu\\), we find the log-likelihood \\(\\ell(\\mu)\\), take the derivative with respect to \\(\\mu\\), set it equal to zero, and solve for \\(\\mu\\):\nFirst, the likelihood function \\(L(\\mu)\\) is:\n\\[\nL(\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nNext, the log-likelihood \\(\\ell(\\mu)\\) is:\n\\[\n\\ell(\\mu) = \\sum_{i=1}^n \\left[\\log\\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\right) - \\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]\n\\]\nNow, differentiate the log-likelihood with respect to \\(\\mu\\):\n\\[\n\\frac{d}{d\\mu} \\ell(\\mu) = \\sum_{i=1}^n \\frac{x_i - \\mu}{\\sigma^2}\n\\]\nSetting this equal to zero and solving for \\(\\mu\\), we get:\n\\[\n\\mu_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nNote that applying the log function to the likelihood helped decompose the product and removed the exponential function, which allowed us to easily solve for the MLE."
  },
  {
    "objectID": "posts/post-with-code/index.html#maximum-likelihood-estimation-using-expectation-maximization-algorithm",
    "href": "posts/post-with-code/index.html#maximum-likelihood-estimation-using-expectation-maximization-algorithm",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "Maximum Likelihood Estimation using Expectation-Maximization Algorithm",
    "text": "Maximum Likelihood Estimation using Expectation-Maximization Algorithm\nThe Sample\nThe sample is made of \\(N\\) independently and identically distributed draws from a mixture of \\(K\\)-dimensional multivariate normal distributions.\nThe Mixture\nThe joint probability density function of the \\(n\\)-th observation is:\n\\[\np(x_n) = \\sum_{d=1}^{D} \\pi_d \\mathcal{N}(x_n; \\mu_d, V_d)\n\\]\nwhere:\n\n\\(\\pi_d\\) is the probability of the \\(d\\)-th component of the mixture;\nthe \\(K \\times 1\\) vector \\(\\mu_d\\) is the mean vector of the \\(d\\)-th component;\nthe \\(K \\times K\\) matrix \\(V_d\\) is the covariance matrix of the \\(d\\)-th component.\n\nThe probabilities of the \\(D\\) components of the mixture are non-negative and sum to 1:\n\\[\n\\sum_{d=1}^{D} \\pi_d = 1\n\\]\nThe covariance matrices \\(V_d\\) are assumed to be positive definite, so that their determinants:\n\\[\n\\text{det}(V_d) &gt; 0\n\\]\nare strictly positive.\nThe Vector of Parameters\nWe will denote by \\(\\theta\\) the vector that gathers all the parameters of the model, that is:\n\\[\n\\theta = \\{\\pi_1, \\dots, \\pi_D, \\mu_1, \\dots, \\mu_D, V_1, \\dots, V_D\\}\n\\]\nThe Mixture as a Latent Variable Model\nWe can write the Gaussian mixture model as a latent-variable model:\n\\[\np(x_n, z_n) = \\prod_{n=1}^{N} \\prod_{d=1}^{D} \\left[\\pi_d \\mathcal{N}(x_n; \\mu_d, V_d)\\right]^{z_{nd}}\n\\]\nwhere:\n\nThe observable variables \\(x_n\\) are conditionally multivariate normal with mean \\(\\mu_d\\) and variance \\(V_d\\):\n\n\\[\nx_n | z_n = d \\sim \\mathcal{N}(x_n; \\mu_d, V_d)\n\\]\n\nThe latent variables \\(z_n\\) have the discrete distribution:\n\n\\[\np(z_n = d) = \\pi_d\n\\]\nfor \\(d = 1, \\ldots, D\\).\nIn the formula above, we have explicitly written the value of the latent variable on which we are conditioning \\((z_n = d)\\). However, in what follows, we will also use the notations \\(p(x_n)\\) and \\(p(x_n | z_n)\\) if the value taken by \\(z_n\\) is implicitly given by the context.\nThe EM Algorithm\nSince we are able to write the Gaussian mixture model as a latent-variable model, we can use the EM algorithm to find the maximum likelihood estimators of its parameters.\nStarting from an initial guess of the parameter vector \\(\\theta_0\\), the algorithm produces a new estimate of the parameter vector \\(\\theta_j\\) at each iteration \\(j\\).\nThe \\(j\\)-th iteration consists of two steps:\n\nThe Expectation step, where we compute the conditional probabilities of the latent variables using the vector \\(\\theta_{j-1}\\) from the previous iteration.\nThe Maximization step, where we maximize the expectation of the complete-data log-likelihood, computed with respect to the conditional probabilities found in the Expectation step. The result of the maximization is a new parameter vector \\(\\theta_j\\).\n\nThe iterations end when a stopping criterion is met (e.g., when the increases in the log-likelihood or the changes in the parameter vector become smaller than a certain threshold). See below for more details.\nSome Notation\nWe denote by \\(x\\) and \\(z\\) the vectors obtained by stacking all the observed and latent variables \\(x_n\\) and \\(z_n\\), respectively.\nMoreover, we use double subscripts for the various parameters. The first subscript denotes the mixture component \\(d\\), and the second one the iteration number \\(j\\).\nFor example, at the \\(j\\)-th iteration, the parameter vector \\(\\theta_j\\) contains an estimate of the covariance matrix of the \\(d\\)-th component of the mixture, which we denote by:\n\\[\nV_{d,j}\n\\]"
  },
  {
    "objectID": "posts/post-with-code/index.html#reference",
    "href": "posts/post-with-code/index.html#reference",
    "title": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION",
    "section": "Reference",
    "text": "Reference\nBonakdarpour, M. (2016, January 22). Introduction to EM: Gaussian Mixture Models. Introduction to em: Gaussian mixture models. https://stephens999.github.io/fiveMinuteStats/intro_to_em.html"
  }
]